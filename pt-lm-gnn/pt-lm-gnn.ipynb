{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90479c14-a7a9-41ac-8121-c67eb99d0370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aaindex import aaindex1\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os,zipfile,pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc13357-e4a4-4c3d-a3af-4bc123efb797",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/zlei/.cache/bio_embeddings/prottrans_bert_bfd/model_directory were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m BERT_EMBEDDER\u001b[38;5;241m=\u001b[39mProtTransBertBFDEmbedder()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbio_embeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprottrans_t5_embedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProtTransT5XLU50Embedder\n\u001b[0;32m----> 5\u001b[0m T5_EMBEDDER\u001b[38;5;241m=\u001b[39m\u001b[43mProtTransT5XLU50Embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhalf_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bio_embeddings/embed/prottrans_t5_embedder.py:66\u001b[0m, in \u001b[0;36mProtTransT5Embedder.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_fallback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_directory, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/utils/import_utils.py:992\u001b[0m, in \u001b[0;36mDummyObject.__getattr__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, key)\n\u001b[0;32m--> 992\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/utils/import_utils.py:980\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    978\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from bio_embeddings.embed.prottrans_bert_bfd_embedder import ProtTransBertBFDEmbedder\n",
    "BERT_EMBEDDER=ProtTransBertBFDEmbedder()\n",
    "\n",
    "from bio_embeddings.embed.prottrans_t5_embedder import ProtTransT5XLU50Embedder\n",
    "T5_EMBEDDER=ProtTransT5XLU50Embedder(half_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540a20c-4c5e-4d6a-a6c2-0671f9cdb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES=aaindex1.record_codes()\n",
    "\n",
    "AMINO_ACIDS=aaindex1.amino_acids()\n",
    "\n",
    "ALL_VALUES={aa:[] for aa in AMINO_ACIDS}\n",
    "\n",
    "for code in tqdm(CODES):\n",
    "    values=aaindex1[code].values\n",
    "    for aa in AMINO_ACIDS:\n",
    "        ALL_VALUES[aa]+=[values[aa]]\n",
    "\n",
    "\n",
    "for k,aa in enumerate(ALL_VALUES.keys()):\n",
    "    vals=np.array(ALL_VALUES[aa]).reshape(1,-1)\n",
    "    if k==0:\n",
    "        pc_embs=vals\n",
    "    else:\n",
    "        pc_embs=np.concatenate([pc_embs,vals],axis=0)\n",
    "\n",
    "PC_EMBS=(pc_embs-np.mean(pc_embs,axis=0).reshape(1,-1))/np.std(pc_embs,axis=0).reshape(1,-1)\n",
    "\n",
    "\n",
    "def create_aaindex_emb(seq):\n",
    "    for k,x in enumerate(seq):\n",
    "        idx=AMINO_ACIDS.index(x)\n",
    "        aa_emb=PC_EMBS[idx].reshape(1,-1)\n",
    "        if k==0:\n",
    "            emb=aa_emb\n",
    "        else:\n",
    "            emb=np.concatenate([emb,aa_emb],axis=0)\n",
    "    return emb\n",
    "\n",
    "def create_bert_emb(seq):\n",
    "    return BERT_EMBEDDER.embed(seq)\n",
    "\n",
    "def create_t5_emb(seq):\n",
    "    return T5_EMBEDDER.embed(seq)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "data_folder=f\"embeddings\"\n",
    "yu_data_folder=\"../datasets/yu_merged/\"\n",
    "\n",
    "\n",
    "\n",
    "LIGANDS=[\"ADP\", \"AMP\", \"ATP\", \"CA\", \"FE\", \"GDP\", \"GTP\", \"HEME\", \"MG\", \"MN\", \"ZN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c8e02-16d5-414b-924e-2d46eb9a76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for LIGAND in LIGANDS:\n",
    "    print(f\"Computing embeddings for {LIGAND}\")\n",
    "    yu_path=os.path.join(yu_data_folder,\"Training_sets\",f\"{LIGAND}_Training.txt\")\n",
    "    train_df=pd.read_csv(yu_path,sep=\";\")\n",
    "    yu_path=os.path.join(yu_data_folder,\"Testing_sets\",f\"{LIGAND}_Validation.txt\")\n",
    "    test_df=pd.read_csv(yu_path,sep=\";\")\n",
    "\n",
    "\n",
    "    with zipfile.ZipFile(f'{data_folder}/Training_sets/all_embs_{LIGAND}_Training.zip','w') as new_zip:\n",
    "        for k in range(train_df.shape[0]):  \n",
    "            all_embs={}\n",
    "            chain=train_df[\"pdb_id\"][k]+\"_\"+train_df[\"chain_id\"][k]\n",
    "            seq=train_df[\"sequence\"][k]\n",
    "            all_embs[\"aaindex\"]=create_aaindex_emb(seq)\n",
    "            all_embs[\"bert\"]=create_bert_emb(seq)\n",
    "            all_embs[\"t5\"]=create_t5_emb(seq)\n",
    "            file=f\"{chain}.p\"\n",
    "            dest=f\"{data_folder}/{file}\"\n",
    "            pickle.dump(all_embs,open(dest,\"wb\"))\n",
    "            new_zip.write(dest,file,compress_type=zipfile.ZIP_BZIP2)\n",
    "            os.remove(dest)\n",
    "    \n",
    "    with zipfile.ZipFile(f'{data_folder}/Testing_sets/all_embs_{LIGAND}_Testing.zip','w') as new_zip:\n",
    "        for k in range(test_df.shape[0]):  \n",
    "            all_embs={}\n",
    "            chain=test_df[\"pdb_id\"][k]+\"_\"+test_df[\"chain_id\"][k]\n",
    "            seq=test_df[\"sequence\"][k]\n",
    "            all_embs[\"aaindex\"]=create_aaindex_emb(seq)\n",
    "            all_embs[\"bert\"]=create_bert_emb(seq)\n",
    "            all_embs[\"t5\"]=create_t5_emb(seq)\n",
    "            file=f\"{chain}.p\"\n",
    "            dest=f\"{data_folder}/{file}\"\n",
    "            pickle.dump(all_embs,open(dest,\"wb\"))\n",
    "            new_zip.write(dest,file,compress_type=zipfile.ZIP_BZIP2)\n",
    "            os.remove(dest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
